### 线性回归模型用到的目标函数

- OLS的目标函数: $J(w) = \sum_{j=1}^N(y_i - f(x_i))^2$
    * 在最小二乘线性回归里，目标函数只包含了训练样本上的残差平方和

- 岭回归的目标函数：$J(w; \lambda) = \sum_{i=1}^N(y_i - f(x_i))^2 + \lambda \sum_{j=1}^D w_j^2)$
    * 目标函数里除了训练集上的残差平方和之外，还加了L2正则

- Lasso回归的目标函数：$J(w;\lambda) = \sum_{i=1}^N (y_i - f(x_i))^2 + \lambda \sum_{j=1}^D |w_j|$
    * 目标函数中是把L2正则换成了L1正则

### 目标函数的最优解

- 给定正则参数(超参数) $\lambda$ 的情况下，目标函数最优解 $\hat{w} = \underset{w}{argmin} J(w)$
    * $\hat{w}$是使得目标函数J(w)最小的 w
- 最优解的必要条件: 一阶导数为0：$\frac{\partial J(w)}{\partial w} = 0$

### OLS的最优解：正规方程组

- OLS目标函数(矩阵形式)

<div align="center">
    <img width="600" src="./screenshot/4.35.jpg">
</div>

### OLS的最优解: Moore-Penrose广义逆

- OLS目标函数： $J(w) = ||y - Xw||_2^2$
- 相当于求：y = Xw
- 如果X为方阵, 可求逆： $w = X^{-1}y$
- 如果𝐗不是方阵，可求Moore-Penrose广义逆: $w = X^\dagger y$
- Moore-Penrose广义逆可采用奇异值分解(Singular Value Decomposition, SVD)实现:

<div align="center">
    <img width="400" src="./screenshot/4.36.jpg">
</div>

### 岭回归的最优解 —— SVD分解

- 岭回归的目标函数为: $J(w) = ||y - Xw||_2^2 + \lambda ||w||_2^2 = (y - Wx)^T(y-Xw) + \lambda w^Tw$
- 偏导数: $\frac{\partial J(w)}{\partial w} = -2X^Ty + 2(X^TX)w + 2\lambda w = 0$ ，得到岭回归的解：$\hat{w}_{Ridge} = (X^TX + \lambda I)^{-1} X^Ty$
- 对比最小二乘的解: $\hat{w}_{OLS} = (X^T)^{-1}X^Ty$
- 岭回归的解： $\hat{w}_{Ridge} = (X^TX + \lambda I)^{-1}X^Ty = (X^TX + \lambda I)^{-1}(X^TX)(X^TX)^{-1}X^Ty = (X^TX + \lambda I)^{-1}(X^TX)\hat{w}_{OLS}$
- $\hat{w}_{Ridge}$在$\hat{w}_{OLS}$的基础上进行了收缩

### 总结

- OLS的解为: $\hat{w}_{OLS} = (X^TX)^{-1}X^Ty$，需要对矩阵$X^TX$求逆
- 当输入特征存在共线性(某些特征可以用其他特征的线形组合表示)，矩阵X是接近不满秩，矩阵$X^TX$接近奇异，求逆不稳定
- 岭回归的解为: $\hat{w}_{Ridge} = (X^TX + \lambda I)^{-1}X^Ty$, 对矩阵$(X^TX + \lambda I)$求逆
- 即使输入特征存在共线性，矩阵X不满秩，矩阵$X^TX$对角线存在等于0或接近于0的元素，但$0 + \lambda ≠ 0$，$(X^TX + \lambda I)$求逆仍可得到稳定解。因此岭回归在输入特征存在共线性的情况仍然能得到稳定解。
- Lasso的无法求得解析解，可用迭代求解