1. 理解回归模型的噪声模型:正态分布，白噪声是一个正态分布
2. 理解极大似然估计
3. 理解最小L2损失和正态分布噪声模型极大似估计的等价性

### 最佳模型

- 基于之前的回归是这样的，给定训练数据D，学习一个从输入x到输出y的映射f, 其实这个f就是我们的目标函数
- 在找最佳模型的时候，我们有一个目标函数，目标函数是希望这个模型在训练集上的损失越小越好，常用的一种损失函数是L2损失
- 因此模型的目标函数可以写成这样 $J(w) = RSS(w) = \sum_{i=1}^N L(y_i, \hat{y}_i) = \sum_{i=1}^N L(y_i, f(x_i)) = \sum_{i=1}^N (y_i - f(x_i))^2$ 
- 这里J是目标函数常用符号，w是模型的参数w，在线性回归模型中，$f(x)=w^Tx$, w知道后，f(x)就知道了
- 采用L2损失作为损失函数的话，另外有一个经常叫的名字叫最小二乘(Ordinary Least Square, OLS)，这个**二**就是平方，然后这个**最小**是指对这个目标函数求极值
- OLS就是指损失函数采用L2损失的回归模型

### 噪声分布

- 在回归任务中，令模型预测值和真实值之间的差异为噪声 $\varepsilon$，假设噪声$\varepsilon$的分布为0均值的正态分布：$\varepsilon \sim N(0, \sigma ^2)$
- 这里的N指Normal，也就是正态分布，$(0, \sigma ^2)$ 是正态分布的参数，0 是均值，$\sigma ^2$是方差
- 如果给噪声做这样一个假设的话，那么数据产生的模型可以写成 $y = f(x) + \varepsilon$
- 是这样的，当给定一个x，那么f(x)就定了，加上一个随机变量 $\varepsilon$, 那么最终y还是一个随机变量，因此它还是一个正态分布 $y|x \sim N(f(x), \sigma^2)$
- 均值变成了f(x), 方差不变

### 极大似然估计

- 由于$y|x \sim N(f(x), \sigma^2)$还是一个正态分布，我们可以知道它的概率密度函数 $p(y|x) = \frac{1}{\sqrt{2\pi}\sigma} exp(-\frac{(y - f(x))^2}{2\sigma^2})$
- log似然函数为：

$$
\ell(f) = logp(D) = \sum_{i=1}^N logp(y_i - f(x_i) | x_i) \\

= \sum_{i=1}^N log [ \frac{1}{\sqrt{2\pi} \sigma} exp(-\frac{(y_i - f(x_i))^2}{2 \sigma^2}) ] \\

= -\frac{N}{2} log(2\pi) - N log \sigma - \sum_{i=1}^N \frac{(y_i - f(x_i))^2}{2 \sigma^2}

$$

- 极大似然估计是说，我们要计算似然函数，$\ell$ 是likelyhood的意思, 这里的D是我们的训练数据，也就是训练数据出现的概率
- 在机器学习中，通常我们假设样本是独立同分布的，所谓**独立**就是指所有数据的联合分布等于每个数据的概率相乘，这里相乘再取log等于分别取log再相加
- 这里$\sum_{i=1}{N}$ 对应的是独立同分布样本
- 我们可知 $\ell(f) = -\frac{N}{2} log(2\pi) - N log \sigma - \sum_{i=1}^N \frac{(y_i - f(x_i))^2}{2 \sigma^2}$
- 在这里f是变量，因为$f(x)=w^Tx$ 可以说f里面的参数w是变量，当f变化时，损失函数的值会变化
- 相对f而言， $-\frac{N}{2} log(2\pi)$ 这一项是常数，求使得$\ell$最大的f的时候，去除常数项不影响极值点的位置
- $- N log \sigma$ 这一项和f没有关系，也可以去掉这个常数项
- 最后只剩下$- \sum_{i=1}^N \frac{(y_i - f(x_i))^2}{2 \sigma^2}$这部分了
- 所以我们要求极大log似然函数，也就是要求极大的$- \sum_{i=1}^N \frac{(y_i - f(x_i))^2}{2 \sigma^2}$
- 也就是说 $- \sum_{i=1}^N \frac{(y_i - f(x_i))^2}{2 \sigma^2}$求最大值，转换成 $\sum_{i=1}^N \frac{(y_i - f(x_i))^2}{2 \sigma^2}$ 求最小值
- 如果把常数项$2\sigma^2$去掉，这个表达式就是残差平方和的式子：$\sum_{i=1}^N(y_i - f(x_i))^2 = RSS(f)$
- 这是残差平方和或者叫做训练集上的L2损失之和
- 当RSS(残差平方和)最小，也就是等价于极大似然估计，换句话说：极大似然估计等价于最小二乘
- 在极大似然的时候，我们假设在给定x的时候，y的分布是一个正态分布，均值是f(x), 方差是$\sigma^2$, 即：$y|x \sim N(f(x), \sigma^2)$
- 数学家高斯采用正态分布对回归误差进行分析，因此正态分布亦被称为高斯分布
- 通过用正态分布来表示误差，就可以用极大似然估计这套理论来分析最小二乘得到最后L2损失最小

### 负log似然损失

- 极大似然估计等价于负log似然最小，因此负log似然也被称为一种损失函数: 负log似然损失
- L2损失也是负log似然损失。
- 分类任务中Logistic回归中采用的损失函数也是负log似然损失

### 关于极大似然估计总结

- 给定数据$D=\{x_i\}_{i=1}^N$，似然函数定义为数据出现的概率
- 通常我们假定数据是独立同分布样本，因此所有数据出现的概率等于每个数据点出现的概率相乘
- 实际计算中，通常对似然函数取log运算(log函数为单调函数，不影响取极值的位置; 很多分布的概率密度函数为指数函数形式，log运算数值计算更稳定)
- 得到log似然函数: $\ell(\theta) = logp(D|\theta) = \sum_{i=1}^N logp(x_i|\theta)$, 其中$\theta$为分布的参数
- 统计中我们需要根据观测数据$D=\{x_i\}_{i=1}^N$，估计分布的参数𝜃，一种常用的参数估计为极大似然估计，即：$\underset{\theta}{argmax} \ell(\theta)$

### 扩展

- 回归任务中L1损失对应的噪声模型是什么分布? L1损失最小也等价于极大似然估计吗?
- 提示: Laplace分布为 $x \sim Lapalce(u,b) = \frac{1}{2b} exp(-\frac{|x-u|}{b})$
- 这里有一个绝对值，结论是L1损失之和最小也等价于极大似然估计，只是这里的噪声模型是Laplace分布
- 历史上拉普拉斯先于高斯研究天体的回归模型的噪声，但是采用的是Laplace分布，由于这个分布在原点处不连续，在优化计算时存在困难
- 反而是高斯最先突破回归问题的求解，历史上也有人把正态分布称为Gause-Laplace分布

### 总结

- 回归模型中的最常用的噪声模型是正态分布的噪声模型
- 如果噪声分布模型是正态分布的话，我们发现极大似然估计等价于最小L2损失之和