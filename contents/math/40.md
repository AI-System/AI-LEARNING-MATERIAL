### 极大似然估计法

- 极大似然估计法是在总体的分布类型已知的条件下所使用的一种参数估计方法
- 它首先是由德国数学家Gauss在1821年提出的，然而，这个方法常归功于英国统计学家Fisher
- Fisher在1922年重新发现了这一方法，并首先研究了这种方法的一些性质

### 极大似然原理

- 一个随机试验有若干个可能结果A，B，C, ...。若在一次试验中，结果A发生，则一般认为试验条件对A最有利，即A发生的概率$P(A/\theta)$最大
- 如: $甲 \left \{\begin{array}{cccc}99 & 红 \\1 & 黑\end{array} \right. , 乙 \left \{\begin{array}{cccc}1 & 红 \\99 & 黑\end{array} \right.$
- 任取1箱从中任取1球，已知取到红球，问最优可能从何箱取$P(红球/甲) = 0.99， P(红球/乙) = 0.01$
- 可见从甲箱取更合理
- 又如，龟兔赛跑，得第一名的最有可能是谁？

**X--离散型**

- 已知X的分布$P(X=x)=p(x, \theta)$, $\theta$未知，样本$(X_1, X_2, ..., X_n)$取到观测值(x_1,x_2,..,x_n)
- $P(A) = P(X_1 = x_1, X_2 = x_2, ..., X_n = x_n)$
    * 独立 $P(X_1 = x_1)P(X_2 = x_2)...P(X_n = x_n)$
    * $X_i$与X同分布 $P(X=x_1)P(X=x_2)...P(X=x_n)=p(x_1, \theta)p(x_2, \theta)...p(x_n, \theta) = \Pi_{i=1}^n p(x_i, \theta)$
    * 给定的样本值$(x_1, x_2, ..., x_n), \Pi_{i=1}^n p(x_i, \theta)$ 是参数$\theta$的函数，称为似然函数，记为：$L(\theta)$, 即：$L(\theta) = \Pi_{i=1}^n p(x_i; \theta)$
- 结构：n项连乘，总体分布 $p(x, \theta) \to p(x_i, \theta), i = 1,2,...,n$, $P(A) = L(\theta)$, 随$\theta$变而变，A已经发生，由极大似然原理，$L(\theta)$达到最大
- 所以，$\theta$的最合理估计值$\hat{theta}$应满足：$L(\hat{\theta})$为最大值

**定义**

- 对给定的样本值$x_1, x_2, ..., x_n$ 若$\hat{\theta}(x_1, x_2, ... x_n)$ 满足 $L(\hat{\theta}) = \mathop{max}\limits_{\theta}(L(\theta))$
- 称：$\hat{\theta}(x_1, x_2, ..., x_n)$为$\theta$的极大似然估计值，$\hat{\theta}(X_1, X_2, ..., X_n)$ 为$\theta$的极大似然估计量
- 求$\hat{\theta}$, 即求$L(\theta)$的最大值点问题
- 方法一：
    * 若$L(\theta)$为可导函数，解方程 $\frac{dL(\theta)}{d\theta} = 0$, 得到 $\hat{\theta} = \hat{\theta}(X_1, X_2, ..., X_n)$
    * $f(x) > 0, ln[f(x)]$单调性相同，从而最大值点相同
    * $L(\theta) = \Pi_{i=1}^n p(x_i;\theta)$, n项连乘，求导麻烦，$ln[L(\theta)]$(对数似然函数) n项相加，求导简单
    * 从而，求得$L(\theta)$最大值点就转为求$ln[L(\theta)]$的最大值点
- 方法二：
    * 解方程 $\frac{dln[L(\theta)]}{d\theta} = 0$, 得到$\hat{\theta}$

**连续型总体似然函数的求法**

- 设X为连续型总体，其概率密度为：$f(x;\theta)$ 其中$\theta$未知
- 对来自总体的样本$(X_1, X_2, ..., X_n)$, 其观测值为 $(x_1, x_2, ..., x_n)$, 作为与总体X同分布且相互独立的n维随机变量，样本的联合概率密度为：
- $f(x_1, x_2, ..., x_n) = f_{X_1}(x_1)f_{X_2}(x_2)...f_{X_n}(x_n) = f(x_1, \theta) · f(x_2, \theta) \cdots f(x_n, \theta) = \Pi_{i=1}^n f(x_i, \theta)$
- 于是，样本 $(X_1, X_2, \cdots, X_n)$ 落入点 $(x_1, x_2, \cdots, x_n)$ 邻域内的概率为：$\Pi_{i=1}^n f(x_i, \theta) \triangle x_i$ 由极大似然原理, 最合理的$\theta$的估计值$\hat{\theta}$应该是使$\Pi_{i=1}^n f(x_i, \theta) \triangle x_i$ 达到最大，由于$\triangle x_i$是不依赖于$\theta$的增量，所以我们只需使似然函数$L(\theta) = \Pi_{i=1}^n f(x_i, \theta)$ 达到最大
- 求$\hat{\theta}$的步骤
    * (1)写出$L(\theta)$
    * (2)取对数$ln L(\theta)$
    * (3)解方程$\frac{dln[L(\theta)]}{d\theta} = 0$, 得到$\hat{\theta}$

**例1**

- 一个罐子里装有黑球和白球，有放回地抽取n个球，发现有k个黑球。试求罐子里黑球数与白球数之比R的极大似然估计量
- 分析
    * 设罐中装有a只黑球，b只白球，则$R=\frac{a}{b}$
    * 设$x_i=\left (\begin{array}{cccc}1 & \text{第i次摸到黑区} \\0 & \text{第i次摸到白球}\end{array} \right.$ i = 1,...,n.
    * 则$X_1, ..., X_n$是总体$X \sim b(1,p)$的样本，
    * 其中 $p = P\{ X_i = 1 \} = \frac{a}{a+b} = \frac{R}{1+R}$
    * 似然函数$L(R)=\Pi_{i=1}^np(x_i) = \Pi_{i=1}^n p^{x_i} (1-p)^{1-x_i}$
    * $=\Pi_{i=1}^n (\frac{R}{1+R})^{x_i} (1 - \frac{R}{1+R})^{1-x_i} = (\frac{R}{1+R})^{\sum_{i=1}^n x_i} (1 - \frac{R}{1+R})^{n - \sum_{i=1}^n x_i}$
    * 则 $ln L = \sum_{i=1}^n x_i ln(\frac{R}{1+R}) + (n - \sum_{i=1}^n x_i) ln(\frac{1}{1 + R})$
    * $=\sum_{i=1}^n x_i [lnR - ln(1 + R)] - (n - \sum_{i=1}^n x_i) ln(1 + R)]$
    * 令 $\frac{d lnL}{dR} = 0$, 则 $\sum_{i=1}^n x_i (\frac{1}{R} - \frac{1}{1+R}) - (n - \sum_{i=1}^n x_i)\frac{1}{1+R} = 0$
    * 解得 $\hat{R} = \frac{\sum_{i=1}^n x_i}{n - \sum_{i=1}^n x_i} = \frac{k}{n - k}$

**例2**

- 设$X \sim N(\mu, \sigma^2)$, $\mu$已知，$\sigma^2$为未知参数，$x_1, ..., x_n$是来自X的一个样本值，求$\sigma^2$的极大似然估计量
- 分析
    * X的概率密度为：$f(x;\sigma^2) = \frac{1}{\sqrt{2\pi} \sigma} exp\{ - \frac{1}{2\sigma^2} (x - \mu)^2 \}$
    * 似然函数为：$L(\sigma^2) = \Pi_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} exp\{ - \frac{1}{2\sigma^2} (x_i - \mu)^2 \}$
    * $=(2\pi \sigma^2)^{- \frac{n}{2}} exp \{ -\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2 \}$
    * $ln L = -\frac{n}{2} ln(2\pi) - \frac{n}{2} ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2$
    * $ln L = -\frac{n}{2} ln(2\pi) - \frac{n}{2} ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2$
    * $\frac{d lnL}{d \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2$
    * 令：$\frac{d lnL}{d \sigma^2} = 0$,
    * 得似然方程：$- \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2 = 0$
    * 解此方程，$\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2$
    * 因此，$\sigma^2$的极大似然估计量为：$\hat{\theta}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2$

**例3**

- 设$X \sim N(\mu, \sigma^2)$, $\mu, \sigma^2$为未知参数，$x_1, ..., x_n$ 是来自X的一个样本值，求$\mu, \sigma^2$的极大似然估计量
- 分析
    * X的概率密度为：$f(x;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma} exp\{ - \frac{1}{2\sigma^2} (x - \mu)^2 \}$
    * 似然函数为：$L(\mu, \sigma^2) = \Pi_{i=1}^n \frac{1}{\sqrt{2\pi} \sigma} exp\{ - \frac{1}{2\sigma^2}(x_i - \mu)^2 \}$
    * $=(2\pi \sigma^2)^{-\frac{n}{2}} e^{-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^2}}$
    * $ln L = -\frac{n}{2} ln(2\pi) - \frac{n}{2} ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n(x_i - \mu)^2$
    * $ln L = - \frac{n}{2} ln(2\pi) - \frac{n}{2} ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2$
    * 令：$\left \{ \begin{array}{cccc} \frac{\partial ln L}{\partial \mu} = 0 \\ \frac{\partial ln L}{\partial \sigma^2} = 0 \end{array}  \right.$ 即：$\left \{ \begin{array}{cccc} \frac{1}{\sigma^2} \sum_{i=1}^n (x_i - \mu) = 0 \\ - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (x_i - \mu)^2 = 0 \end{array}  \right.$
    * 解得：$\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}, \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2$
    * 故，$\mu, \sigma^2$的极大似然估计量为：$\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}, \ \ \  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$